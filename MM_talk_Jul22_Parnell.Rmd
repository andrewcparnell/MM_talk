---
title: "<br>Anomaly detection and other artificial intelligence tools in industry 4.0" 
author: "Andrew C Parnell"
date: "11th July 2022 <br> https://is.gd/Parnell_MM_talk"
output: 
  ioslides_presentation: 
    css: my-style.css
    transition: slower
    autosize: true
    widescreen: yes
    linkcolor: white
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.align = 'center')
```

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script>

<script>
    $(document).ready(function() {
      $('slide:not(.title-slide, .backdrop, .segue)').append('<footer label=\"https://is.gd/Parnell_MM_talk\"></footer>');    
    })
</script>

<style>
  footer:after {
    content: attr(label);
    font-size: 12pt;
    position: absolute;
    bottom: 25px;
    left: 60px;
    line-height: 1.9;
  }
</style>



## Contents

<br>

<span style="color:red">1. An introduction to artificial intelligence</span><br>

<span style="color:blue">2. Some examples of artificial intelligence used by the I-Form centre</span><br>

![](https://github.com/andrewcparnell/MM_talk/raw/main/img/I-Form-Logo.GIF){width=70%}


## What is ML and AI?

- Machine Learning (ML) and Artificial Intelligence (AI) are methods to make a computer take input data and make <span style="color:red">_predictions_</span>.
- In the recent past, a huge explosion in data has led to the <span style="color:blue">_Big Data_</span revolution:

    - Volume
    - Variety
    - Veracity
    - Velocity

## Examples of AI: Character and voice recognition

<center>

![](https://github.com/andrewcparnell/MM_talk/raw/main/img/mnistdigits.jpg){width=60%}

</center>

## Genomics

<center>

![](https://github.com/andrewcparnell/MM_talk/raw/main/img/snp_plot.gif){width=85%}

</center>

## Industry 4.0 (and perhaps 5.0)

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/RenAM500M.jpg){width=70%}
</center>

## What makes these data all the same?

- They each have a __target__ that we're trying to predict (a digit, the size of a tumour, an aspect of a tool)
- They each have a large number of __features__ which we can use to predict the target (pixels, genes, sensor data)
- The data can be written as __rectangular data structures__ (like an Excel spreadsheet)
- We make the predictions using a __learner__. There are lots of different types, from linear regression to deep neural networks

## Testing performance

- To determine the performance of our AI models we usually separate the data into a _training set_ and a _test set_
- Taken together the data set, the target variable and the features are known as a __task__
- We __train__ the learner on the training set and then __predict__ on the test set
- Seeing as we know the target values for the test set we can determine how far away they are from the truth

## A workflow in R

\small
```{r, eval = FALSE}
library(mlr)
# Make a task
task_1 = makeRegrTask(data = my_data_set, target = 'y')
# Create training and test sets
test_set = sample(n,size = n/4)
train_set = (1:n)[-test_set]
# Set up a leaner
learner_1 = makeLearner('regr.h2o.deeplearning')
# Train it on the training data
train_1 = train(learner = learner_1, task = task_1, subset = train_set)
# Predict on the test set data
pred_1 = predict(object = train_1, task = task_1, subset = test_set)
# Check performance
performance(pred_1)
```

## A reminder of the terminology

- A _task_ is the data set, the _features_, and the _target_ variable
- A _training_ and _test_ set are just the rows of the data that we will train and test our model on
- A _learner_ is a black box that aims to predict the target from the features
- The _predictions_ on the test set can be compared to the true target values to judge the performance of the learner

## The foundation of all the black boxes: least squares linear regression

- We aim to estimate $f()$ in:
$$y = f(X) + e$$
- The simplest useful model is a linear regression model:
$$f = a + b_1 x_1 + b_2 x_2 + b_3 x_3 + \ldots $$

- We might minimise the least squares loss function:
$$\sum (y - f(X))^2$$
We find the `best' values of $a$, $b_1$, $b_2$ etc that minimise the loss

## From linear regression to deep learning

Make the $b$ terms depend on other hidden $b$ terms to produce richer functional behaviour

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/nn_pic.jpg){width=70%}
</center>

## Types of AI

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/ML_types.png){width=80%}
</center>

## Running the digits data through R

```{r, include = FALSE}
source('https://gist.githubusercontent.com/brendano/39760/raw/22467aa8a5d104add5e861ce91ff5652c6b271b6/gistfile1.txt')
load_mnist()
library(dplyr)
library(readr)
mnist = cbind(rbind(train$x, test$x), 
              c(train$y, test$y)) %>% as.data.frame
colnames(mnist) = c(paste0('pix',1:784), 'digit')
#install.packages('h2o')
library(h2o)
#h2o.shutdown()
# h2o.init(nthreads = -1)
library(mlr)
```

```{r}
show_digit(train$x[5,])
```

## What does the data look like?

```{r}
mnist[1:6, c(405:410, 785)] # 0 = white, 255 = black
```

Each image is 28 by 28 pixels

## Fitting a deep learning classifier

\small 
```{r, include = FALSE}
# Make a task
task_mn = makeClassifTask(data = mnist, target = 'digit')
# Create training and test sets
train_set = 1:60000
test_set = 60001:70000
# Set up a leaner
learner_dl = makeLearner('classif.h2o.deeplearning',
                         predict.type = 'prob')
# Train it on the training data
# train_dl = train(learner = learner_dl, task = task_mn,
#                  subset = train_set)
# saveRDS(train_dl, file = 'train_dl.rds')
train_dl <- readRDS(file = 'train_dl.rds')
# Predict on the test set data
# pred_dl = predict(object = train_dl, task = task_mn,
#                  subset = test_set)
# # Check performance
# saveRDS(pred_dl, file = 'pred_dl.rds')
pred_dl <- readRDS(file = 'pred_dl.rds')
#performance(pred_dl) # 3.2% incorrect
```

```{r, eval = FALSE}
# Make a task and set up training/test sets
task_mn = makeClassifTask(data = mnist, target = 'digit')
train_set = 1:60000
test_set = 60001:70000
# Set up a leaner
learner_dl = makeLearner('classif.h2o.deeplearning',
                         predict.type = 'prob')
# Train it on the training data
train_dl = train(learner = learner_dl, task = task_mn, subset = train_set)
pred_dl = predict(object = train_dl, task = task_mn, subset = test_set)
# Check performance
```
```{r}
print(performance(pred_dl))
```

## How well did it do?

```{r, echo = FALSE}
probs = pred_dl$data[,3:12]
class = apply(probs, 1, which.max) - 1
print(table(pred_dl$data$truth, class))
```

## What about other ML models?

- With `mlr` it is easy to train other models in the same way
```{r, eval = FALSE}
learner_rf = makeLearner('classif.h2o.randomforest',
                         predict.type = 'prob')
# Train it on the training data
train_rf = train(learner = learner_rf, task = task_mn, 
                 subset = train_set)
# Predict on the test set data
pred_rf = predict(object = train_rf, task = task_mn, 
                  subset = test_set)
print(performance(pred_rf))
```

## Assessing performance of ML models

- A good model should have predicted values close to the true values, or predicted classes close to the true classes. Some models will also produce uncertainties (e.g. intervals) or probabilities which can help determine the quality of the fit
- The usual metrics people use are the _root mean squared error_ (for regression problems), the _misclassification rate_ (for classification problems)
- Other more advanced metrics for classifiers include the _receiver operator characteristic curve_ and _precision-recall curve_. These work on the probabilities of each observation being in a particular class

## Neural network extensions

- Convolutional and recurrent neural networks designed for image or time series data
- Transfer learning where we use a pre-trained neural network that might have seen similar data to that which we are modelling
- Active learning which allows for the user to train the model (e.g. provide labels) on data that the model is struggling to predict
- Probabilistic uncertainty when we require more than just predictions
- There are many more!

# <span style = "color:white;position:relative;top:50px;">Part 2: AI and I-Form</span> {data-background=https://github.com/andrewcparnell/MM_talk/raw/main/img/code.png data-background-size=cover}

## Common problems in AI and manufacturing

- Extracting data from the machine
- Fast and accurate analysis of high-dimensional data
- Human labelling of the data stream 
- Interpreting the analytics for the user so that they can take action

## A recommender system

<center>
<video width="640" height="480" controls>
  <source src="https://github.com/andrewcparnell/MM_talk/raw/main/img/recsys1.mp4" type="video/mp4">
</video>
</center>

## Recommender system structure

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/RecSys_schematic.png){width=65%}
</center>

## Recommender system interface

<video width="640" height="480" controls>
  <source src="https://github.com/andrewcparnell/MM_talk/raw/main/img/RecSys_Live.mp4" type="video/mp4">
</video>

## Anomaly detection 1: Control charts 

<center>
![](https://github.com/anhoej/qicharts2/raw/master/docs/articles/qicharts2_files/figure-html/unnamed-chunk-10-1.png){width=65%}
</center>

## Anomaly detection 2: Model-based clustering

<center>
![](https://mclust-org.github.io/mclust/articles/mclust_files/figure-html/unnamed-chunk-10-3.png){width=65%}
</center>

## Anomaly detection 3: STRAY

<center>
![](https://github.com/pridiltal/stray/raw/master/man/figures/README-datad-1.png){width=65%}
</center>

## Anomaly detection 4: Autoencoders

<center>
![](https://miro.medium.com/max/1200/1*nqzWupxC60iAH2dYrFT78Q.png){width=65%}
</center>

## Using STRAY to classify pores

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Doyle_1.jpg){width=33%}
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Doyle_2.jpg){width=33%}
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Doyle_3.jpg){width=33%}
</center>

## Using image detection to classify pores from images

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Sheehan_1.png){width=33%}
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Sheehan_2.jpg){width=33%}
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Sheehan_3.jpg){width=33%}
</center>

## How to learn more about AI in manufacturing

- SCREENSHOT OF THE GITHUB REPO AND TRAINING SCHEME

## A final word on reproducibility

- The importance of it

## Summary

- In the near future we will be regularly streaming large amounts of data from AM machines and analysing it in real time to identify defects as they occur
- The tools of AI are important to learn and will help you ride the wave of Industry 4.0/5.0
- Come and join the I-Form centre in Phase 2 if you want to help build on these ideas

## Resources and funding

Autoencoders: [Medium](https://medium.com/@birla.deepak26/autoencoders-76bb49ae6a8f);

__All materials: [Github repo](https://github.com/andrewcparnell/MM_talk)__

This work was supported by a Science Foundation Ireland Career Development Award grant 17/CDA/4695 and Research Centre Award 16/RC/3872

<center>

![](https://www.sfi.ie/sfi-logo-and-guidelines/SFI_logo_2017__Dual(long)_CMYK.png){width=50%}

<br> 

<font size="6"><span style="color:red;">Thank you!</span></font>

</center>