---
title: "<br>Anomaly detection and other artificial intelligence tools in industry 4.0" 
author: "Andrew C Parnell"
date: "11th July 2022 <br> https://is.gd/Parnell_MM_talk"
output: 
  ioslides_presentation: 
    css: my-style.css
    transition: slower
    autosize: true
    widescreen: yes
    linkcolor: white
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.align = 'center')
```

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script>

<script>
    $(document).ready(function() {
      $('slide:not(.title-slide, .backdrop, .segue)').append('<footer label=\"https://is.gd/Parnell_MM_talk\"></footer>');    
    })
</script>

<style>
  footer:after {
    content: attr(label);
    font-size: 12pt;
    position: absolute;
    bottom: 25px;
    left: 60px;
    line-height: 1.9;
  }
</style>



## Contents

<br>

<span style="color:red">1. An introduction to artificial intelligence</span><br>

<span style="color:blue">2. Some examples of artificial intelligence used by the I-Form centre</span><br>


## What is ML and AI?

- Machine Learning (ML) and Artificial Intelligence (AI) are methods to make a computer take input data and make _predictions_.
- In the recent past, a huge explosion has led to the _Big Data_ revolution:

    - Volume
    - Variety
    - Veracity
    - Velocity

## Examples of ML: Character and voice recognition

<center>

![](https://github.com/andrewcparnell/MM_talk/raw/main/img/mnistdigits.jpg){width=70%}

</center>

## Genomics

![](https://github.com/andrewcparnell/MM_talk/raw/main/img/snp_plot.jpg){width=70%}

## Industry 4.0 (and perhaps 5.0)

![](https://github.com/andrewcparnell/MM_talk/raw/main/img/minstdigits.jpg){width=70%}

## What makes these data all the same?

- They each have a __target__ that we're trying to predict (a digit, the size of a tumour, an aspect of a tool)
- They each have a large number of __features__ which we can use to predict the target (pixels, genes, sensor data)
- If we write the target as $y$ and the features as $X$ all these can be written as rectangular data structures (like an Excel spreadsheet)
- We make the predictions using a __learner__. There are lots of different types, from linear regression to deep neural networks

## Testing performance

- To determine the performance of our AI models we usually separate the data into a _training set_ and a _test set_
- Taken together the data set, the target variable and the features are known as a __task__
- We __train__ the learner on the training set and then __predict__ on the test set
- Seeing as we know the target values for the test set we can determine how far away they are from the truth

## A workflow in R

\small
```{r, eval = FALSE}
library(mlr)
# Make a task
task_1 = makeRegrTask(data = my_data_set, 
                      target = 'y')
# Create training and test sets
test_set = sample(n, 
                  size = n/4)
train_set = (1:n)[-test_set]
# Set up a leaner
learner_1 = makeLearner('regr.h2o.deeplearning')
# Train it on the training data
train_1 = train(learner = learner_1, task = task_1, 
                subset = train_set)
# Predict on the test set data
pred_1 = predict(object = train_1, task = task_1, 
                 subset = test_set)
# Check performance
performance(pred_1)
```

## A reminder of the terminology

- A _task_ is the data set, the _features_, and the _target_ variable
- A _training_ and _test_ set are just the rows of the data that we will train and test our model on
- A _learner_ is a black box that aims to predict the target from the features
- The _predictions_ on the test set can be compared to the true target values to judge the performance of the learner

## The foundation of all the black boxes: least squares linear regression

- We aim to estimate $f()$ in:
$$y = f(X) + e$$
- The simplest possible useful model is a linear regression model:
$$f = a + b_1 x_1 + b_2 x_2 + b_3 x_3 + \ldots $$

- We might minimise the least squares loss function:
$$\sum (y - f(X))^2$$
We find the `best' values of $a$, $b_1$, $b_2$ etc that minimise the loss

## From linear regression to deep learning

Make the $b$ terms depend on other hidden $b$ terms to produce richer functional behaviour

\centering \includegraphics[width=\textwidth]{nn_pic.jpg}

## Types of AI

- Strictly speaking, the previously mentioned problems are all part of a sub-class of AI called __supervised learning__. They are supervised because they have a target variable. If the target variable is continuous this is commonly called _regression_ (e.g. the genetics data) whilst if it is discrete it is known as _classification_ (e.g. the digits data)
- Another version of ML occurs when you have no target variable. In this case this is commonly known as __unsupervised learning__. 

## Running the digits data through R

```{r, include = FALSE}
source('https://gist.githubusercontent.com/brendano/39760/raw/22467aa8a5d104add5e861ce91ff5652c6b271b6/gistfile1.txt')
load_mnist()
library(dplyr)
library(readr)
mnist = cbind(rbind(train$x, test$x), 
              c(train$y, test$y)) %>% as.data.frame
colnames(mnist) = c(paste0('pix',1:784), 'digit')
#install.packages('h2o')
library(h2o)
#h2o.shutdown()
# h2o.init(nthreads = -1)
library(mlr)
```

```{r}
show_digit(train$x[5,])
```

## What does the data look like?

```{r}
mnist[1:6, c(405:410, 785)] # 0 = white, 255 = black
```


## Fitting a deep learning classifier

\small 
```{r, include = FALSE}
# Make a task
task_mn = makeClassifTask(data = mnist, 
                          target = 'digit')
# Create training and test sets
train_set = 1:60000
test_set = 60001:70000
# Set up a leaner
learner_dl = makeLearner('classif.h2o.deeplearning',
                         predict.type = 'prob')
# Train it on the training data
# train_dl = train(learner = learner_dl, task = task_mn,
#                  subset = train_set)
# saveRDS(train_dl, file = 'train_dl.rds')
train_dl <- readRDS(file = 'train_dl.rds')
# Predict on the test set data
# pred_dl = predict(object = train_dl, task = task_mn,
#                  subset = test_set)
# # Check performance
# saveRDS(pred_dl, file = 'pred_dl.rds')
pred_dl <- readRDS(file = 'pred_dl.rds')
#performance(pred_dl) # 3.2% incorrect
```

```{r, eval = FALSE}
# Make a task and set up training/test sets
task_mn = makeClassifTask(data = mnist, 
                          target = 'digit')
train_set = 1:60000
test_set = 60001:70000
# Set up a leaner
learner_dl = makeLearner('classif.h2o.deeplearning',
                         predict.type = 'prob')
# Train it on the training data
train_dl = train(learner = learner_dl, task = task_mn,
                 subset = train_set)
pred_dl = predict(object = train_dl, task = task_mn,
                  subset = test_set)
# Check performance
```
```{r}
print(performance(pred_dl))
```

## How well did it do?

```{r, echo = FALSE}
probs = pred_dl$data[,3:12]
class = apply(probs, 1, which.max) - 1
print(table(pred_dl$data$truth, class))
```

## What about other ML models?

- With `mlr` it is easy to train other models in the same way
```{r, eval = FALSE}
learner_rf = makeLearner('classif.h2o.randomforest',
                         predict.type = 'prob')
# Train it on the training data
train_rf = train(learner = learner_rf, task = task_mn, 
                 subset = train_set)
# Predict on the test set data
pred_rf = predict(object = train_rf, task = task_mn, 
                  subset = test_set)
print(performance(pred_rf))
```

## Assessing performance of ML models

- A good model should have predicted values close to the true values, or predicted classes close to the true classes. Some models will also produce uncertainties (e.g. intervals) or probabilities which can help determine the quality of the fit
- The usual ones people use are the _root mean squared error_ (for regression problems), the _misclassification rate_ (for classification problems)
- Other more advanced ones for classifiers include the _receiver operator characteristic curve_ and _precision-recall curve_. These work on the probabilities of each observation being in a particular class

## Neural network extensions

- Convolutional and recurrent neural networks designed for image or time series data
- Transfer learning where we use a pre-trained neural network that might have seen similar data to that which we are modelling
- Active learning which allows for the user to train the model (e.g. provide labels) on data that the model is struggling to predict
- There are many more!

# <span style = "color:white;position:relative;top:50px;">Part 2: AII and I-Form</span> {data-background=https://github.com/andrewcparnell/MM_talk/raw/main/img/code.png data-background-size=cover}

## A recommender system

<video controls>
  <source src="https://github.com/andrewcparnell/MM_talk/raw/main/img/recsys1.mp4" type="video/mp4">
</video>



## Resources and funding

Monticchio: [Wikipedia](https://upload.wikimedia.org/wikipedia/commons/b/b1/Monticchio_Laghi.jpg);
gifs: [Giphy](https://giphy.com);
AMS machine: [Wikipedia](https://upload.wikimedia.org/wikipedia/commons/f/f0/1_MV_accelerator_mass_spectrometer.jpg);
Roadworks background: [flickr/Jeremy Segrott](https://www.flickr.com/photos/126337928@N05/);
Confidence in the IPCC: [IPCC.ch](https://www.ipcc.ch/site/assets/uploads/2017/08/AR5_Uncertainty_Guidance_Note.pdf)
Baby turtle: [flickr/Pedro Szekely](https://www.flickr.com/photos/pedrosz/);
[BART generator](http://www.ranzey.com/generators/bart/index.html);
Anger: [flickr/Saurabh Vyas](https://www.flickr.com/photos/vyassaurabh411/);
Walking: [flickr/gato-gato-gato](https://www.flickr.com/photos/gato-gato-gato/)
__All materials: [Github repo](https://github.com/andrewcparnell/reproducibility_talk_jan22)__

This work was supported by a Science Foundation Ireland Career Development Award grant 17/CDA/4695


<center>

![](https://www.sfi.ie/sfi-logo-and-guidelines/SFI_logo_2017__Dual(long)_CMYK.png){width=50%}

<br> 

<font size="6"><span style="color:red;">Thank you!</span></font>

</center>

# <span style = "color:orangered;position:relative;top:50px;">What is AI?</span> {data-background=https://github.com/andrewcparnell/MM_talk/raw/main/img/code.png data-background-size=cover}
