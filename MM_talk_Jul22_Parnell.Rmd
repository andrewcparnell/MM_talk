---
title: "<br>Anomaly detection and other artificial intelligence tools in industry 4.0" 
author: "Andrew C Parnell"
date: "11th July 2022 <br> https://is.gd/Parnell_MM_talk"
output: 
  ioslides_presentation: 
    css: my-style.css
    transition: slower
    autosize: true
    widescreen: yes
    linkcolor: white
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.align = 'center')
```

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script>

<script>
    $(document).ready(function() {
      $('slide:not(.title-slide, .backdrop, .segue)').append('<footer label=\"is.gd/Parnell_MM_talk\"></footer>');    
    })
</script>

<style>
  footer:after {
    content: attr(label);
    font-size: 12pt;
    position: absolute;
    bottom: 25px;
    left: 60px;
    line-height: 1.9;
  }
</style>



## Contents

<br>

<span style="color:red">1. An introduction to artificial intelligence</span><br>

<span style="color:blue">2. Some examples of artificial intelligence used by the I-Form centre</span><br>

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/I-Form-Logo.GIF){width=40%}
</center>


## What is ML and AI?

- Machine Learning (ML) and Artificial Intelligence (AI) are methods to make a computer take input data and make <span style="color:red">_predictions_</span>.
- In the recent past, a huge explosion in data has led to the <span style="color:blue">_Big Data_</span> revolution:

    - Volume
    - Variety
    - Veracity
    - Velocity

## Examples of AI: Character and voice recognition

<center>

![](https://github.com/andrewcparnell/MM_talk/raw/main/img/mnistdigits.jpg){width=60%}

</center>

## Genomics

<center>

![](https://github.com/andrewcparnell/MM_talk/raw/main/img/snp_plot.gif){width=85%}

</center>

## Industry 4.0 (and perhaps 5.0)

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/RenAM500M.jpg){width=60%}
</center>

## What makes these data all the same?

- They each have a <span style="color:red">_target_</span> that we're trying to predict (a digit, the size of a tumour, an aspect of a tool)
- They each have a large number of <span style="color:blue">_features_</span> which we can use to predict the target (pixels, genes, sensor data)
- The data can be written as <span style="color:green">_rectangular data structures_</span> (like an Excel spreadsheet)
- We make the predictions using a <span style="color:orange">_learner_</span>. There are lots of different types, from linear regression to deep neural networks

## Testing performance

- To determine the performance of our AI models we usually separate the data into a <span style="color:red">_training set_</span> and a <span style="color:blue">_test set_</span>
- Taken together the data set, the target variable and the features are known as a <span style="color:green">_task_</span>
- We <span style="color:orange">_train_</span> the learner on the training set and then <span style="color:purple">_predict_</span> on the test set
- Seeing as we know the target values for the test set we can determine how far away they are from the truth

## A workflow in R

\small
```{r, eval = FALSE}
library(mlr)
# Make a task
task_1 = makeRegrTask(data = my_data_set, target = 'y')
# Create training and test sets
test_set = sample(n,size = n/4)
train_set = (1:n)[-test_set]
# Set up a leaner
learner_1 = makeLearner('regr.h2o.deeplearning')
# Train it on the training data
train_1 = train(learner = learner_1, task = task_1, subset = train_set)
# Predict on the test set data
pred_1 = predict(object = train_1, task = task_1, subset = test_set)
# Check performance
performance(pred_1)
```

## A reminder of the terminology

- A _task_ is the data set, the _features_, and the _target_ variable
- A _training_ and _test_ set are just the rows of the data that we will train and test our model on
- A _learner_ is a black box that aims to predict the target from the features
- The _predictions_ on the test set can be compared to the true target values to judge the performance of the learner

## Learners 101: Linear regression

- We aim to estimate $f()$ in:
$$y = f(X) + e$$
- The simplest useful model is a <span style="color:red">_linear regression_</span> model:
$$f = a + b_1 x_1 + b_2 x_2 + b_3 x_3 + \ldots $$

- We might minimise the <span style="color:blue">_least squares loss function_</span>:
$$\sum (y - f(X))^2$$
We find the `best' values of $a$, $b_1$, $b_2$ etc that <span style="color:green">_minimise the loss_</span>

## From linear regression to deep learning

Make the $b$ terms depend on other <span style="color:red">_hidden_</span> $b$ terms to produce richer functional behaviour

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/nn_pic.jpg){width=70%}
</center>

## Types of AI

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/ML_types.png){width=80%}
</center>

## Running the digits data through R

```{r, include = FALSE}
source('https://gist.githubusercontent.com/brendano/39760/raw/22467aa8a5d104add5e861ce91ff5652c6b271b6/gistfile1.txt')
load_mnist()
library(dplyr)
library(readr)
mnist = cbind(rbind(train$x, test$x), 
              c(train$y, test$y)) %>% as.data.frame
colnames(mnist) = c(paste0('pix',1:784), 'digit')
#install.packages('h2o')
library(h2o)
#h2o.shutdown()
# h2o.init(nthreads = -1)
library(mlr)
```

```{r}
show_digit(train$x[5,])
```

## What does the data look like?

```{r}
mnist[1:6, c(405:410, 785)] # 0 = white, 255 = black
```

Each image is 28 by 28 pixels

## Fitting a deep learning classifier

\small 
```{r, include = FALSE}
# Make a task
task_mn = makeClassifTask(data = mnist, target = 'digit')
# Create training and test sets
train_set = 1:60000
test_set = 60001:70000
# Set up a leaner
learner_dl = makeLearner('classif.h2o.deeplearning',
                         predict.type = 'prob')
# Train it on the training data
# train_dl = train(learner = learner_dl, task = task_mn,
#                  subset = train_set)
# saveRDS(train_dl, file = 'train_dl.rds')
train_dl <- readRDS(file = 'train_dl.rds')
# Predict on the test set data
# pred_dl = predict(object = train_dl, task = task_mn,
#                  subset = test_set)
# # Check performance
# saveRDS(pred_dl, file = 'pred_dl.rds')
pred_dl <- readRDS(file = 'pred_dl.rds')
#performance(pred_dl) # 3.2% incorrect
```

```{r, eval = FALSE}
# Make a task and set up training/test sets
task_mn = makeClassifTask(data = mnist, target = 'digit')
train_set = 1:60000
test_set = 60001:70000
# Set up a leaner
learner_dl = makeLearner('classif.h2o.deeplearning',
                         predict.type = 'prob')
# Train it on the training data
train_dl = train(learner = learner_dl, task = task_mn, subset = train_set)
pred_dl = predict(object = train_dl, task = task_mn, subset = test_set)
# Check performance
```
```{r}
print(performance(pred_dl))
```

## How well did it do?

```{r, echo = FALSE}
probs = pred_dl$data[,3:12]
class = apply(probs, 1, which.max) - 1
print(table(pred_dl$data$truth, class))
```

## What about other ML models?

- With `mlr` it is easy to train other models in the same way
```{r, eval = FALSE}
learner_rf = makeLearner('classif.h2o.randomforest',
                         predict.type = 'prob')
# Train it on the training data
train_rf = train(learner = learner_rf, task = task_mn, 
                 subset = train_set)
# Predict on the test set data
pred_rf = predict(object = train_rf, task = task_mn, 
                  subset = test_set)
print(performance(pred_rf))
```

## Assessing performance of ML models

- A good model should have predicted values close to the true values, or predicted classes close to the true classes. Some models will also produce uncertainties (e.g. intervals) or probabilities which can help determine the quality of the fit
- The usual metrics people use are the <span style="color:red">_root mean squared error_</span> (for regression problems), the <span style="color:blue">_misclassification rate_</span>(for classification problems)
- Other more advanced metrics for classifiers include the <span style="color:green">_receiver operator characteristic curve_</span> and <span style="color:orange">_precision-recall curve_</span>. These work on the probabilities of each observation being in a particular class

## Neural network extensions

- <span style="color:red">_Convolutional and recurrent_</span> neural networks designed for image or time series data
- <span style="color:blue">_Transfer learning_</span> where we use a pre-trained neural network that might have seen similar data to that which we are modelling
- <span style="color:green">_Active learning_</span> which allows for the user to train the model (e.g. provide labels) on data that the model is struggling to predict
- <span style="color:orange">_Probabilistic uncertainty_</span> when we require more than just predictions
- There are many more!

# <span style = "color:white;position:relative;top:50px;">Part 2: AI and I-Form</span> {data-background=https://github.com/andrewcparnell/MM_talk/raw/main/img/code.png data-background-size=cover}

## Common problems in AI and manufacturing

- <span style="color:red">_Extracting data_</span> from the machine
- Fast and accurate analysis of <span style="color:blue">high-dimensional_</span> data
- <span style="color:green">_Human labelling_</span> of the data stream 
- <span style="color:orange">_Interpreting_</span> the analytics for the user so that they can take action

## A recommender system

<center>
<video width="640" height="480" controls>
  <source src="https://github.com/andrewcparnell/MM_talk/raw/main/img/recsys1.mp4" type="video/mp4">
</video>
</center>

## Recommender system structure

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/RecSys_schematic.png){width=65%}
</center>

## Recommender system interface

<center>
<video width="640" height="480" controls>
  <source src="https://github.com/andrewcparnell/MM_talk/raw/main/img/RecSys_Live.mp4" type="video/mp4">
</video>
</center>

## Anomaly detection 1: Control charts 

<center>
![](https://github.com/anhoej/qicharts2/raw/master/docs/articles/qicharts2_files/figure-html/unnamed-chunk-10-1.png){width=85%}
</center>

## Anomaly detection 2: Model-based clustering

<center>
![](https://mclust-org.github.io/mclust/articles/mclust_files/figure-html/unnamed-chunk-10-3.png){width=65%}
</center>

## Anomaly detection 3: STRAY

<center>
![](https://github.com/pridiltal/stray/raw/master/man/figures/README-datad-1.png){width=55%}
</center>

## Anomaly detection 4: Autoencoders

<center>
![](https://miro.medium.com/max/1200/1*nqzWupxC60iAH2dYrFT78Q.png){width=75%}
</center>

## Using STRAY to classify pores

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Doyle_1.jpg){width=20%}
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Doyle_2.jpg){width=20%}
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Doyle_3.jpg){width=50%}
</center>

## Using image detection to classify pores from images

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Sheehan_1.png){width=20%}
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Sheehan_2.jpg){width=30%}
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Sheehan_3.jpg){width=25%}
</center>

## How to learn more about AI in manufacturing

<center>
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Github_1.jpg){width=40%}
![](https://github.com/andrewcparnell/MM_talk/raw/main/img/Github_2.jpg){width=40%}
</center>

## Summary

- In the near future we will be <span style="color:red">_regularly streaming_</span> large amounts of data from AM machines and analysing it in real time to identify defects as they occur
- The tools of AI are <span style="color:blue">_important to learn_</span> and will help you ride the wave of Industry 4.0/5.0
- Come and join the <span style="color:green">_I-Form centre_</span> in Phase 2 if you want to help work on extending these ideas

## Resources and funding

Autoencoders: [Medium](https://medium.com/@birla.deepak26/autoencoders-76bb49ae6a8f);

__All materials: [Github repo](https://github.com/andrewcparnell/MM_talk)__

This work was supported by a Science Foundation Ireland Career Development Award grant 17/CDA/4695 and Research Centre Award 16/RC/3872

<center>

![](https://www.sfi.ie/sfi-logo-and-guidelines/SFI_logo_2017__Dual(long)_CMYK.png){width=50%}

<br> 

<font size="6"><span style="color:red;">Thank you!</span></font>

</center>